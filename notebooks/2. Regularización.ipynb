{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regularización\n",
    "\n",
    "En este notebook se revisarán los conceptos de:\n",
    "\n",
    "1. Regularización en regresión\n",
    "    1. Ridge regression\n",
    "    2. Lasso \n",
    "2. Regularización en clasificación\n",
    "    1. Regresión logística\n",
    "\n",
    "Primero cargamos librerías y funciones necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos una función para representar el resultado del ajuste\n",
    "def plot_decision_boundary(X,y,h,model):\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = .05  # step size in the mesh\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Zd = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Zd = Zd.reshape(xx.shape)\n",
    "    \n",
    "    Zp = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1] \n",
    "    Zp = Zp.reshape(xx.shape)\n",
    "    \n",
    "    # Error de clasificación\n",
    "    ypred = model.predict(X)\n",
    "    acc = accuracy_score(y,ypred)\n",
    "    \n",
    "    plt.figure(1, figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "    plt.axis([x_min, x_max, y_min, y_max])\n",
    "    plt.contour(xx, yy, Zd, levels=[0], linewidths=2)\n",
    "    plt.contourf(xx, yy, Zd,cmap=cm, alpha=.5)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "    msg = 'FRONTERA DECISION\\n Acc: %0.2g' % acc\n",
    "    plt.title(msg)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "    plt.axis([x_min, x_max, y_min, y_max])\n",
    "    #plt.contour(xx, yy, Zp, levels=[0], linewidths=2)\n",
    "    plt.contourf(xx, yy, Zp,cmap=cm, alpha=.5)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "    msg = 'PROBABILIDAD\\n Acc: %0.2g' % acc\n",
    "    plt.title(msg)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos una función para representar el resultado del ajuste\n",
    "def plot_decision_boundary_poly(X,y,h,model,poly):\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = .05  # step size in the mesh\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    XX = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Zd = model.predict(poly.fit_transform(XX))\n",
    "    Zd = Zd.reshape(xx.shape)\n",
    "    \n",
    "    Zp = model.predict_proba(poly.fit_transform(XX))[:,1] \n",
    "    Zp = Zp.reshape(xx.shape)\n",
    "    \n",
    "    # Error de clasificación\n",
    "    ypred = model.predict(poly.fit_transform(X))\n",
    "    acc = accuracy_score(y,ypred)\n",
    "    \n",
    "    plt.figure(1, figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "    plt.axis([x_min, x_max, y_min, y_max])\n",
    "    plt.contour(xx, yy, Zd, levels=[0], linewidths=2)\n",
    "    plt.contourf(xx, yy, Zd,cmap=cm, alpha=.5)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "    msg = 'FRONTERA DECISION\\n Acc: %0.2g' % acc\n",
    "    plt.title(msg)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "    plt.axis([x_min, x_max, y_min, y_max])\n",
    "    plt.contour(xx, yy, Zp, levels=[0], linewidths=2)\n",
    "    plt.contourf(xx, yy, Zp,cmap=cm, alpha=.5)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "    msg = 'PROBABILIDAD\\n Acc: %0.2g' % acc\n",
    "    plt.title(msg)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Regularización sobre algoritmos de regresión.\n",
    "\n",
    "Vamos a trabajar sobre el mismo ejemplo que vimos en la clase anterior para entender el concepto de regularización. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_train = 10\n",
    "N_test  = 100\n",
    "\n",
    "# función verdadera g(x)\n",
    "x = np.linspace(0,1,N_test)\n",
    "g_x = np.cos(1.5*np.pi*x)\n",
    "\n",
    "# proceso y\n",
    "np.random.seed(0) # para asegurar reproducibilidad\n",
    "epsilon = np.random.randn(N_test) * 0.2\n",
    "y = g_x + epsilon\n",
    "\n",
    "# Datos: D = {x_i,y_i}, obtenemos una muestra\n",
    "idx = np.random.randint(0,N_test,N_train)\n",
    "x_i = x[idx]\n",
    "y_i = y[idx]\n",
    "\n",
    "# dibujamos la función g(x), y el conjunto de datos x_i,y_i\n",
    "plt.plot(x,g_x,'r',label='g(x)')\n",
    "plt.plot(x_i,y_i,'b.',label='y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, ajustamos un modelo (regresión lineal) muy complejo sobre estos datos, de la forma:\n",
    "\n",
    "$$f_{\\omega}(x) = \\omega_0 + \\sum_{j=1}^{10}\\omega_j x^j$$\n",
    "\n",
    "el cual tiene 10 coeficientes ($\\omega_1,\\ldots,\\omega_{10}$). Este modelo seguramente sufrirá de overfitting.\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Entrena un algoritmo de regresión lineal y calcula el error cuadrático medio del conjunto de test\n",
    "</div>\n",
    "\n",
    "Error cuadrático medio: $$MSE = \\frac{1}{N}\\sum_{i=1}^N \\left(y^{(i)}-\\hat{y}^{(i)}\\right)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6a8fd02919b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# predicción\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Calcula el MSE: TU CÓDIGO AQUÍ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lr' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "degree = 10\n",
    "\n",
    "# features\n",
    "poly    = PolynomialFeatures(degree) # generamos x^j\n",
    "X_train = poly.fit_transform(x_i.reshape(-1, 1))\n",
    "y_train = y_i\n",
    "\n",
    "X_test = poly.fit_transform(x.reshape(-1, 1))\n",
    "y_test = y\n",
    "\n",
    "# Entrena modelo: TU CÓDIGO AQUÍ\n",
    "\n",
    "###\n",
    "\n",
    "# predicción\n",
    "y_hat = lr.predict(X_test)\n",
    "\n",
    "# Calcula el MSE: TU CÓDIGO AQUÍ\n",
    "\n",
    "##\n",
    "\n",
    "plt.plot(x,g_x,'r',label='$y$')\n",
    "plt.plot(x_i,y_i,'b.',label='$y_i$')\n",
    "plt.plot(x,y_hat,'g',label='$\\hat{y}$')\n",
    "plt.title('Grado: %i\\nMSE:%.2f'%(degree,mse))\n",
    "plt.legend()\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "plt.show()\n",
    "\n",
    "# Mostramos los coeficientes del modelo\n",
    "print('w0: ', lr.intercept_)\n",
    "\n",
    "w = lr.coef_\n",
    "norm_w2 = np.dot(w,w.T) # no se tiene en cuenta el intercepto\n",
    "\n",
    "coef_names = ['w' + str(i) + ': ' for i in range(1,degree+1)]\n",
    "for f,wi in zip(coef_names,w):\n",
    "    print(f,wi)\n",
    "\n",
    "print('\\n||w||_2^2 = %.2g'%norm_w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Ridge regression\n",
    "\n",
    "Se refiere al modelo de regresión lineal con penalización sobre la magnitud de los coeficientes\n",
    "\n",
    "$$\\min_{\\boldsymbol{\\omega}}|| \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\omega}||_2^2 + \\alpha ||\\boldsymbol{\\omega} ||_2^2$$\n",
    "\n",
    "como medida contra el [*overfitting*](https://en.wikipedia.org/wiki/Overfitting)\n",
    "\n",
    "El modelo de [*ridge regression*](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html), tiene un parámetro libre ($\\alpha$) que hemos de fijar a priori. En otras palabras, tenemos que dar un valor a $\\alpha$ para que el modelo calcule los coeficientes $\\boldsymbol{\\omega}$. A tener en cuenta:\n",
    "\n",
    "* Si $\\alpha = 0$, entonces el resultado coincide con un modelo de [regresión lineal](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "* Si $\\alpha \\to \\infty$, entonces el valor de todos los coeficientes será nulo.\n",
    "\n",
    "Por tanto, para buscar el valor de $\\alpha$ adecuado, tendremos que barrer valores en una escala que cubra valores muy pequeños y valores elevados. Para ello, normalmente se utiliza escala logarítmica aumentando progresivamente el orden  de magnitud. Como ejemplo, podríamos barrer lambda utilizando los siguientes valores $\\alpha = \\{10^{-3},0.01,0.1,1,10,100,1000\\}$, que en escala logarítmica queda como $\\log_{10}({\\alpha}) = \\{-3,-2,-1,0,1,2,3\\}$\n",
    "\n",
    "Vamos a implementar el algoritmo de *ridge regression* variando los valores de $\\alpha$, y viendo cómo esta variación afecta a los coeficientes $\\boldsymbol{\\omega}$ del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "#model\n",
    "alpha = 1e-14\n",
    "ridge = Ridge(alpha = alpha).fit(X_train,y_train)\n",
    "w = ridge.coef_\n",
    "norm_w2 = np.dot(w,w.T)\n",
    "    \n",
    "# predicción\n",
    "y_hat = ridge.predict(X_test)\n",
    "\n",
    "# error\n",
    "error_test = np.mean(np.power(y - y_hat,2)) \n",
    "\n",
    "plt.plot(x,g_x,'r',label='$y$')\n",
    "plt.plot(x_i,y_i,'b.',label='$y_i$')\n",
    "plt.plot(x,y_hat,'g',label='$\\hat{y}$')\n",
    "plt.title('Grado: %i, MSE:%.2f\\nalpha: %g, $||w||_2^2$ = %.2g'%(degree,error_test,alpha,norm_w2))\n",
    "plt.legend()\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "plt.show()\n",
    "\n",
    "coef_names = ['w' + str(i) + ': ' for i in range(1,degree+1)]\n",
    "\n",
    "for f,wi in zip(coef_names,w):\n",
    "    print(f,wi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Varía los valores de $\\alpha$ y comprueba cómo afecta al resultado, ¿Cómo varían los coeficientes del modelo?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representamos ahora el valor de los coeficientes del modelo y su norma para distintos valores del parámetro de regularización\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Entrene un modelo Ridge para distintos valores de $\\alpha$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 20\n",
    "alphas = np.logspace(-10, -3, n_alphas)\n",
    "\n",
    "coefs = []\n",
    "\n",
    "norm2_coefs = []\n",
    "for a in alphas:\n",
    "    #### TU CÓDIGO AQUÍ\n",
    "    \n",
    "    \n",
    "    #####\n",
    "    coefs.append(ridge.coef_)\n",
    "    norm2_coefs.append(np.dot(ridge.coef_,ridge.coef_.T))\n",
    "\n",
    "# Display results\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('$w_i$')\n",
    "plt.title('Coeficientes en función de la regularización')\n",
    "plt.axis('tight')\n",
    "\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(alphas, norm2_coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('$||\\mathbf{w}||^2_2$')\n",
    "plt.title('Norma de los coeffs en función de la regularización')\n",
    "plt.axis('tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: ¿Cuál es el valor óptimo de $\\alpha$?\n",
    "</div>\n",
    "\n",
    "Para responder a este ejercicio de forma correcta debemos probar con distintos valores de $\\alpha$ a partir de los datos, pero ¿cómo calculamos este valor? De nuevo, acudimos a la validación cruzada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Por convención, GridSearchCV siempre intenta MAXIMIZAR los resultados, por lo que\n",
    "# funciones de coste, como MSE, tienen que ir negadas: https://github.com/scikit-learn/scikit-learn/issues/2439\n",
    "# Por eso aparece neg_mean_squared_error y por eso hay luego un -1 multiplicando\n",
    "\n",
    "alpha_vector = np.logspace(-15,1,25)\n",
    "param_grid = {'alpha': alpha_vector }\n",
    "grid = GridSearchCV(Ridge(), scoring= 'neg_mean_squared_error', param_grid=param_grid, cv = 5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "scores = -1*np.array(grid.cv_results_['mean_test_score'])\n",
    "plt.semilogx(alpha_vector,scores,'-o')\n",
    "plt.xlabel('alpha',fontsize=16)\n",
    "plt.ylabel('5-Fold MSE')\n",
    "#plt.ylim((0, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Descomente la línea plt.ylim((0, 1))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo ahora con el valor óptimo de $\\alpha$ que hemos encontrado con validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_optimo = grid.best_params_['alpha']\n",
    "ridge = Ridge(alpha = alpha_optimo).fit(X_train,y_train)\n",
    "\n",
    "# predicción\n",
    "y_hat = ridge.predict(X_test)\n",
    "w = ridge.coef_\n",
    "norm_w2 = np.dot(w,w.T)\n",
    "\n",
    "# error\n",
    "error_test = np.mean(np.power(y - y_hat,2)) \n",
    "\n",
    "\n",
    "plt.plot(x,g_x,'r',label='$y$')\n",
    "plt.plot(x_i,y_i,'b.',label='$y_i$')\n",
    "plt.plot(x,y_hat,'g',label='$\\hat{y}$')\n",
    "plt.title('Grado: %i, MSE:%.2f\\nalpha: %g, $||w||_2^2$ = %.2g'%(degree,error_test,alpha_optimo,norm_w2))\n",
    "plt.legend()\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "plt.show()\n",
    "\n",
    "coef_names = ['w' + str(i) + ': ' for i in range(1,degree+1)]\n",
    "\n",
    "for f,wi in zip(coef_names,w):\n",
    "    print(f,wi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Lasso \n",
    "\n",
    "Se refiere al modelo de regresión lineal con penalización (norma 1) sobre la magnitud de los coeficientes\n",
    "\n",
    "$$\\min_{\\boldsymbol{\\omega}}|| \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\omega}||_2^2 + \\alpha ||\\boldsymbol{\\omega} ||_1$$\n",
    "\n",
    "donde $||\\boldsymbol{\\omega} ||_1 = |\\omega_1| + |\\omega_2| + \\ldots + |\\omega_D|$, siendo $D$ el número de atributos ($\\omega_0$ no se tiene en cuenta en esta penalización).\n",
    "\n",
    "Con esta formulación el algoritmo Lasso permite activar/desactivar coeficientes, de tal forma que se desactivan primero los coeficienes asociados a los atributos que menos influyen en la función de coste (función a minimizar anterior). Con ello:\n",
    "\n",
    "1. Se previene el overfitting, al poder utilizar modelos con menos variables (las desactivamos)\n",
    "2. Se gana interpretabilidad, al poder ver cómo evolucionan las variables supervivientes.\n",
    "\n",
    "La activación y desactivación de variables está determinada por el parámetro de regularización $\\alpha$, de la misma forma que sucede con el algoritmo Ridge:\n",
    "\n",
    "* Si $\\alpha = 0$, entonces el resultado coincide con un modelo de [regresión lineal](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "* Si $\\alpha \\to \\infty$, entonces el valor de todos los coeficientes será nulo.\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Repite los análisis anteriores (Ridge), para el algoritmo [Lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 20\n",
    "alphas = np.logspace(-10, 0, n_alphas)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_vector = np.logspace(-10,1,25)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_optimo = grid.best_params_['alpha']\n",
    "lasso = Lasso(alpha = alpha_optimo).fit(X_train,y_train)\n",
    "# ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: A la vista de los resultados, ¿qué diferencias entre Ridge y Lasso destacarías?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Regularización sobre algoritmos de clasificación\n",
    "\n",
    "Algunos algoritmos de clasificación también tienen un parámetro que permite controlar su complejidad. En el caso de [regresión logística](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), la función de coste a minimizar es de la forma:\n",
    "\n",
    "$$J(\\boldsymbol{\\omega}) = -\\sum_{i=1}^N y^{(i)}\\log{(\\theta(\\mathbf{x}^{(i)}))} + (1-y^{(i)})\\log{\\left(1-\\theta(\\mathbf{x}^{(i)})\\right)}  = \\sum_{i=1}^N \\log{\\left(1+ e^{-y^{(i)}\\boldsymbol{\\omega}^T\\mathbf{x}^{(i)}}\\right)}  $$\n",
    "\n",
    "donde $\\theta(\\mathbf{x}^{(i)}) = \\frac{1}{1+e^{-\\boldsymbol{\\omega}^T\\mathbf{x}^{(i)}}}$. En su versión \"regularizada\", la función de coste pasa a ser:\n",
    "\n",
    "$$J(\\boldsymbol{\\omega}) = \\frac{1}{2}||\\boldsymbol{\\omega}||_2^2 + C \\sum_{i=1}^N \\log{\\left(1+ e^{-y^{(i)}\\boldsymbol{\\omega}^T\\mathbf{x}^{(i)}}\\right)}$$\n",
    "\n",
    "El **coste $C$** es el parámetro libre que permite controlar la complejidad del algoritmo, penalizando los errores que se comenten en clasificación. Este parámetro supone un compromiso entre la exactitud de la solución y la complejidad del algoritmo, en ese sentido es similar al parámetro de regularización $\\alpha$ que utilizamos en Ridge y Lasso. En este caso, de forma intuitiva podemos decir que **$C$ se comporta como  $1/\\alpha$**. Así:\n",
    "\n",
    "- Cuanto mayor es $C$ (menor es $\\alpha$), más penalizamos los errores en clasificación y la frontera se ajusta mucho a los datos (en el caso extremo se ajustará perfectamente). Riesgo de overfitting pero con potencial menor error de clasificación.\n",
    "\n",
    "\n",
    "- Cuanto menor es $C$ (mayor es $\\alpha$), menos penalizamos los errores en clasificación y tenderemos hacia modelos más sencillos (fronteras menos ajustadas, menor riesgo de overfitting pero potencialmente con más error de clasificación)\n",
    "\n",
    "NOTA: por defecto, $C=1$ en scikit-learn.\n",
    "\n",
    "# 2.2 Regularización sobre algoritmos de clasificación\n",
    "\n",
    "Algunos algoritmos de clasificación también tienen un parámetro que permite controlar su complejidad. En el caso de [regresión logística](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), la función de coste a minimizar es de la forma:\n",
    "\n",
    "$$J(\\boldsymbol{\\omega}) = -\\sum_{i=1}^N y^{(i)}\\log{(\\theta(\\mathbf{x}^{(i)}))} + (1-y^{(i)})\\log{\\left(1-\\theta(\\mathbf{x}^{(i)})\\right)}  = \\sum_{i=1}^N \\log{\\left(1+ e^{-y^{(i)}\\boldsymbol{\\omega}^T\\mathbf{x}^{(i)}}\\right)}  $$\n",
    "\n",
    "donde $\\theta(\\mathbf{x}^{(i)}) = \\frac{1}{1+e^{-\\boldsymbol{\\omega}^T\\mathbf{x}^{(i)}}}$. En su versión \"regularizada\", la función de coste pasa a ser:\n",
    "\n",
    "$$J(\\boldsymbol{\\omega}) = \\frac{1}{2}||\\boldsymbol{\\omega}||_2^2 + C \\sum_{i=1}^N \\log{\\left(1+ e^{-y^{(i)}\\boldsymbol{\\omega}^T\\mathbf{x}^{(i)}}\\right)}$$\n",
    "\n",
    "El **coste $C$** es el parámetro libre que permite controlar la complejidad del algoritmo, penalizando los errores que se comenten en clasificación. Este parámetro supone un compromiso entre la exactitud de la solución y la complejidad del algoritmo, en ese sentido es similar al parámetro de regularización $\\alpha$ que utilizamos en Ridge y Lasso. En este caso, de forma intuitiva podemos decir que **$C$ se comporta como  $1/\\alpha$**. Así:\n",
    "\n",
    "- Cuanto mayor es $C$ (menor es $\\alpha$), más penalizamos los errores en clasificación y la frontera se ajusta mucho a los datos (en el caso extremo se ajustará perfectamente). Riesgo de overfitting pero con potencial menor error de clasificación.\n",
    "\n",
    "\n",
    "- Cuanto menor es $C$ (mayor es $\\alpha$), menos penalizamos los errores en clasificación y tenderemos hacia modelos más sencillos (fronteras menos ajustadas, menor riesgo de overfitting pero potencialmente con más error de clasificación)\n",
    "\n",
    "NOTA: por defecto, $C=1$ en scikit-learn.\n",
    "\n",
    "![](./figuras/Sesgo_varianza_parametros.png)\n",
    "\n",
    "\n",
    "\n",
    "Veamos cómo funciona $C$ sobre los ejemplos de clasificación que hemos visto anteriormente.\n",
    "\n",
    "\n",
    "\n",
    "Veamos cómo funciona $C$ sobre los ejemplos de clasificación que hemos visto anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo1\n",
    "ejemplo1 = pd.read_csv(\"./data/ex2data1.txt\", sep=\",\", header=None, names=['x1', 'x2','label'])\n",
    "\n",
    "# ejemplo2\n",
    "ejemplo2 = pd.read_csv(\"./data/ex2data2.txt\", sep=\",\", header=None, names=['x1', 'x2','label'])\n",
    "\n",
    "# ejemplo 3: Problema XOR \n",
    "np.random.seed(0)\n",
    "\n",
    "# -- parameters\n",
    "N     = 800\n",
    "mu    = 1.5      # Cambia este valor\n",
    "sigma = 1      # Cambia este valor\n",
    "\n",
    "# variables auxiliares\n",
    "unos = np.ones(int(N/4))\n",
    "random4 = sigma*np.random.randn(int(N/4),1)\n",
    "random2 = sigma*np.random.randn(int(N/2),1)\n",
    "\n",
    "# -- features\n",
    "y3 = np.concatenate([-1*unos,       unos,          unos,         -1*unos]) \n",
    "X1 = np.concatenate([-mu + random4, mu + random4, -mu + random4, mu + random4])\n",
    "X2 = np.concatenate([+mu + random2,               -mu + random2])\n",
    "X3 = np.hstack((X1,X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(ejemplo1['x1'], ejemplo1['x2'], c=ejemplo1['label'], cmap=cm_bright)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "plt.title('Ejemplo 1')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(ejemplo2['x1'], ejemplo2['x2'], c=ejemplo2['label'], cmap=cm_bright)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "plt.title('Ejemplo 2')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(X3[:,0], X3[:,1], c=y3, cmap=cm_bright)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "plt.title('Ejemplo 3')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# preparamos los datos\n",
    "data1 = ejemplo1.values\n",
    "X1 = data1[:,0:2]\n",
    "y1 = data1[:,-1]\n",
    "\n",
    "# creamos el modelo\n",
    "lr1 = LogisticRegression()\n",
    "\n",
    "# ajustamos con los datos disponibles\n",
    "lr1.fit(X1,y1)\n",
    "plot_decision_boundary(X1,y1,0.05,lr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C grande (ANTES DE EJECUTAR TRATA DE PENSAR CÓMO SERÁ EL RESULTADO!)\n",
    "lr1 = LogisticRegression(C = 1000)\n",
    "lr1.fit(X1,y1)\n",
    "plot_decision_boundary(X1,y1,0.05,lr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C pequeño (ANTES DE EJECUTAR TRATA DE PENSAR CÓMO SERÁ EL RESULTADO!)\n",
    "lr1 = LogisticRegression(C = 0.4)\n",
    "lr1.fit(X1,y1)\n",
    "plot_decision_boundary(X1,y1,0.05,lr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Aplica el algoritmo de regresión logística sobre los ejemplos anteriores 2 y 3, variando el valor de $C$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero cuidado, no hagamos trampas, ¿cómo tenemos que seleccionar el valor de $C$ adecuado para el problema?\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Calcula el valor óptimo de $C$ para el ejemplo 3\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "poly     = PolynomialFeatures(2)\n",
    "X3poly   = poly.fit_transform(X3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X3poly, y3, test_size = 0.3, shuffle = True, random_state = 0)\n",
    "\n",
    "vectorC = np.logspace(-10, 2, 20)\n",
    "param_grid = {'C': vectorC}\n",
    "grid = GridSearchCV(LogisticRegression(fit_intercept=False), scoring= 'accuracy', param_grid=param_grid, cv = 5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "print(\"test-set score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
    "\n",
    "scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "plt.semilogx(vectorC,scores,'-o')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('5-Fold ACC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Representa la frontera de separación para el valor de $C$ óptimo\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu código aquí\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pesar de que la búsqueda de parámetros libres nos proporcione un valor óptimo, quizá éste está demasiado ajustado, por lo tanto es recomendable representar el resultado, en la medida de lo posible, e interpretarlo de forma correcta.\n",
    "\n",
    "Casi todos los algoritmos de machine learning tienen un parámetro que controla la complejidad del mismo, y tenemos que conocer cómo afecta al resultado. Eso sí, para calcular su valor, siempre utilizaremos una estrategia adecuada de selección del modelo (normalmente validación cruzada). Y siempre, siempre, dejamos el conjunto de test en el cofre del tesoro."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
